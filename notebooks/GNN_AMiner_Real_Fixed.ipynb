{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ“ GNN Project - Real AMiner Author Network\n",
    "## Complete Self-Contained Implementation\n",
    "\n",
    "**Dataset:** AMiner Author Network (Real Data)\n",
    "**Tasks:**\n",
    "- ðŸŽ¯ Node Classification: Research field prediction\n",
    "- ðŸ”— Link Prediction: Author collaboration prediction\n",
    "\n",
    "**Models:** GAT, GCN, GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages (uncomment to install)\n",
    "# !pip install torch torchvision\n",
    "# !pip install torch-geometric\n",
    "# !pip install matplotlib pandas numpy scikit-learn requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GCNConv, SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from pathlib import Path\n",
    "import json\n",
    "import requests\n",
    "import gzip\n",
    "import tarfile\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Download & Process Real AMiner Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AMiner-Author.txt found\n"
     ]
    }
   ],
   "source": [
    "def download_aminer():\n",
    "    \"\"\"Use existing AMiner-Author.txt dataset\"\"\"\n",
    "    \n",
    "    data_dir = Path('data/aminer')\n",
    "    author_file = data_dir / 'AMiner-Author.txt'\n",
    "    \n",
    "    if author_file.exists():\n",
    "        print('âœ… AMiner-Author.txt found')\n",
    "        return author_file\n",
    "    else:\n",
    "        print('âŒ AMiner-Author.txt not found')\n",
    "        print('ðŸ’¡ Creating synthetic dataset...')\n",
    "        return None\n",
    "\n",
    "aminer_file = download_aminer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“– Parsing AMiner Author data (limiting to 10000 authors)...\n",
      "Parsed 9000 authors...\n",
      "âœ… Parsed 10000 authors with 68682 unique topics\n",
      "ðŸ“Š Using top 500 topics for features\n",
      "ðŸ”— Creating edges based on shared research topics...\n",
      "ðŸ·ï¸  Creating research field labels...\n",
      "ðŸ“Š Created graph: 10000 authors, 67018 edges, 10 fields\n"
     ]
    }
   ],
   "source": [
    "def parse_aminer(author_file, max_authors=10000):\n",
    "    \"\"\"Parse AMiner Author data and create author collaboration graph\"\"\"\n",
    "    \n",
    "    if author_file is None:\n",
    "        print('Using synthetic data fallback...')\n",
    "        num_nodes = max_authors\n",
    "        X = torch.randn(num_nodes, 128)\n",
    "        X = F.normalize(X, p=2, dim=1)\n",
    "        y = torch.randint(0, 40, (num_nodes,))\n",
    "        edges = []\n",
    "        for i in range(num_nodes):\n",
    "            for _ in range(np.random.randint(3, 10)):\n",
    "                j = np.random.randint(0, num_nodes)\n",
    "                if i != j:\n",
    "                    edges.append([i, j])\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
    "        return X, edge_index, y, num_nodes, 40\n",
    "    \n",
    "    print(f'ðŸ“– Parsing AMiner Author data (limiting to {max_authors} authors)...')\n",
    "    \n",
    "    authors = []\n",
    "    author_topics = []\n",
    "    all_topics = set()\n",
    "    \n",
    "    with open(author_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        current_author = {}\n",
    "        count = 0\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if line.startswith('#index'):\n",
    "                if current_author and 'topics' in current_author:\n",
    "                    authors.append(current_author)\n",
    "                    author_topics.append(current_author['topics'])\n",
    "                    count += 1\n",
    "                    if count >= max_authors:\n",
    "                        break\n",
    "                    if count % 1000 == 0:\n",
    "                        print(f'\\rParsed {count} authors...', end='')\n",
    "                current_author = {'index': int(line.split()[1])}\n",
    "            elif line.startswith('#n'):\n",
    "                current_author['name'] = line[3:].strip()\n",
    "            elif line.startswith('#pc'):\n",
    "                current_author['paper_count'] = int(line.split()[1]) if len(line.split()) > 1 else 0\n",
    "            elif line.startswith('#cn'):\n",
    "                current_author['citation_count'] = int(line.split()[1]) if len(line.split()) > 1 else 0\n",
    "            elif line.startswith('#hi'):\n",
    "                current_author['h_index'] = int(line.split()[1]) if len(line.split()) > 1 else 0\n",
    "            elif line.startswith('#t'):\n",
    "                topics = [t.strip() for t in line[3:].strip().split(';') if t.strip()]\n",
    "                current_author['topics'] = topics\n",
    "                all_topics.update(topics)\n",
    "    \n",
    "    print(f'\\nâœ… Parsed {len(authors)} authors with {len(all_topics)} unique topics')\n",
    "    \n",
    "    topic_counter = Counter([t for topics in author_topics for t in topics])\n",
    "    top_topics = [topic for topic, _ in topic_counter.most_common(500)]\n",
    "    topic_to_idx = {topic: idx for idx, topic in enumerate(top_topics)}\n",
    "    \n",
    "    print(f'ðŸ“Š Using top {len(top_topics)} topics for features')\n",
    "    \n",
    "    num_authors = len(authors)\n",
    "    X = torch.zeros(num_authors, len(top_topics) + 3)\n",
    "    \n",
    "    for i, author in enumerate(authors):\n",
    "        for topic in author['topics']:\n",
    "            if topic in topic_to_idx:\n",
    "                X[i, topic_to_idx[topic]] = 1\n",
    "        X[i, -3] = min(author.get('paper_count', 0) / 100.0, 1.0)\n",
    "        X[i, -2] = min(author.get('citation_count', 0) / 1000.0, 1.0)\n",
    "        X[i, -1] = min(author.get('h_index', 0) / 50.0, 1.0)\n",
    "    \n",
    "    X = F.normalize(X, p=2, dim=1)\n",
    "    \n",
    "    print('ðŸ”— Creating edges based on shared research topics...')\n",
    "    edges = []\n",
    "    topic_authors = defaultdict(list)\n",
    "    for i, topics in enumerate(author_topics):\n",
    "        for topic in topics:\n",
    "            if topic in topic_to_idx:\n",
    "                topic_authors[topic].append(i)\n",
    "    \n",
    "    edge_set = set()\n",
    "    for topic, author_list in topic_authors.items():\n",
    "        for i in range(len(author_list)):\n",
    "            for j in range(i + 1, min(i + 10, len(author_list))):\n",
    "                src, dst = author_list[i], author_list[j]\n",
    "                if (src, dst) not in edge_set and (dst, src) not in edge_set:\n",
    "                    edges.append([src, dst])\n",
    "                    edges.append([dst, src])\n",
    "                    edge_set.add((src, dst))\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t() if edges else torch.zeros((2, 0), dtype=torch.long)\n",
    "    \n",
    "    print('ðŸ·ï¸  Creating research field labels...')\n",
    "    field_keywords = {\n",
    "        'machine learning': 0, 'deep learning': 0, 'neural network': 0,\n",
    "        'computer vision': 1, 'image processing': 1, 'object detection': 1,\n",
    "        'natural language': 2, 'text mining': 2, 'language model': 2,\n",
    "        'database': 3, 'data mining': 3, 'big data': 3,\n",
    "        'network': 4, 'wireless': 4, 'communication': 4,\n",
    "        'security': 5, 'encryption': 5, 'cryptography': 5,\n",
    "        'algorithm': 6, 'optimization': 6, 'complexity': 6,\n",
    "        'software': 7, 'programming': 7, 'development': 7,\n",
    "        'system': 8, 'operating system': 8, 'distributed': 8,\n",
    "        'robotics': 9, 'control': 9, 'automation': 9,\n",
    "    }\n",
    "    \n",
    "    y = torch.zeros(num_authors, dtype=torch.long)\n",
    "    for i, topics in enumerate(author_topics):\n",
    "        for topic in topics[:3]:\n",
    "            topic_lower = topic.lower()\n",
    "            for keyword, label in field_keywords.items():\n",
    "                if keyword in topic_lower:\n",
    "                    y[i] = label\n",
    "                    break\n",
    "            if y[i] != 0:\n",
    "                break\n",
    "    \n",
    "    num_classes = len(set(field_keywords.values()))\n",
    "    \n",
    "    print(f'ðŸ“Š Created graph: {num_authors} authors, {edge_index.shape[1]} edges, {num_classes} fields')\n",
    "    \n",
    "    return X, edge_index, y, num_authors, num_classes\n",
    "\n",
    "X, edge_index, y, num_authors, num_classes = parse_aminer(aminer_file, max_authors=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Dataset Statistics:\n",
      "Authors: 10,000\n",
      "Connections: 67,018\n",
      "Features: 503\n",
      "Research Fields: 10\n",
      "Train: 6,000 | Val: 2,000 | Test: 2,000\n"
     ]
    }
   ],
   "source": [
    "train_mask = torch.zeros(num_authors, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_authors, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_authors, dtype=torch.bool)\n",
    "\n",
    "indices = torch.randperm(num_authors)\n",
    "train_mask[indices[:int(0.6 * num_authors)]] = True\n",
    "val_mask[indices[int(0.6 * num_authors):int(0.8 * num_authors)]] = True\n",
    "test_mask[indices[int(0.8 * num_authors):]] = True\n",
    "\n",
    "data = Data(x=X, edge_index=edge_index, y=y)\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "print(f'\\nðŸ“Š Dataset Statistics:')\n",
    "print(f'Authors: {data.num_nodes:,}')\n",
    "print(f'Connections: {data.num_edges:,}')\n",
    "print(f'Features: {data.num_features}')\n",
    "print(f'Research Fields: {num_classes}')\n",
    "print(f'Train: {train_mask.sum():,} | Val: {val_mask.sum():,} | Test: {test_mask.sum():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Models defined\n"
     ]
    }
   ],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "print('âœ… Models defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Task 1: Node Classification (Research Field Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_node_classification(model, data, epochs=100, lr=0.01):\n",
    "    model = model.to(device)\n",
    "    data = data.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    history = {'train_loss': [], 'val_acc': []}\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x, data.edge_index)\n",
    "            pred = out.argmax(dim=1)\n",
    "            val_acc = (pred[data.val_mask] == data.y[data.val_mask]).float().mean()\n",
    "            history['train_loss'].append(loss.item())\n",
    "            history['val_acc'].append(val_acc.item())\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc.item()\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f'Epoch {epoch+1}: Loss={loss.item():.4f}, Val Acc={val_acc:.4f}')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)\n",
    "        test_acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean()\n",
    "    \n",
    "    print(f'Final: Test Acc={test_acc:.4f}\\n')\n",
    "    return history, test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GAT...\n",
      "Epoch 20: Loss=0.9758, Val Acc=0.6940\n",
      "Epoch 40: Loss=0.8855, Val Acc=0.6950\n",
      "Epoch 60: Loss=0.8574, Val Acc=0.6925\n",
      "Epoch 80: Loss=0.8395, Val Acc=0.6910\n",
      "Epoch 100: Loss=0.8296, Val Acc=0.6890\n",
      "Final: Test Acc=0.6935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training GAT...')\n",
    "gat = GAT(data.num_features, 128, num_classes)\n",
    "gat_hist, gat_acc = train_node_classification(gat, data, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN...\n",
      "Epoch 20: Loss=1.1252, Val Acc=0.6840\n",
      "Epoch 40: Loss=0.9726, Val Acc=0.6945\n",
      "Epoch 60: Loss=0.9226, Val Acc=0.6895\n",
      "Epoch 80: Loss=0.8940, Val Acc=0.6890\n",
      "Epoch 100: Loss=0.8761, Val Acc=0.6880\n",
      "Final: Test Acc=0.6900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training GCN...')\n",
    "gcn = GCN(data.num_features, 128, num_classes)\n",
    "gcn_hist, gcn_acc = train_node_classification(gcn, data, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GraphSAGE...\n",
      "Epoch 20: Loss=1.0479, Val Acc=0.6925\n",
      "Epoch 40: Loss=0.8892, Val Acc=0.6860\n",
      "Epoch 60: Loss=0.8455, Val Acc=0.6865\n",
      "Epoch 80: Loss=0.8264, Val Acc=0.6865\n",
      "Epoch 100: Loss=0.8154, Val Acc=0.6895\n",
      "Final: Test Acc=0.6895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training GraphSAGE...')\n",
    "sage = GraphSAGE(data.num_features, 128, num_classes)\n",
    "sage_hist, sage_acc = train_node_classification(sage, data, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RESEARCH FIELD CLASSIFICATION RESULTS\n",
      "==================================================\n",
      "    Model  Test Accuracy\n",
      "      GAT         0.6935\n",
      "      GCN         0.6900\n",
      "GraphSAGE         0.6895\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame({'Model': ['GAT', 'GCN', 'GraphSAGE'], 'Test Accuracy': [gat_acc, gcn_acc, sage_acc]})\n",
    "print('='*50)\n",
    "print('RESEARCH FIELD CLASSIFICATION RESULTS')\n",
    "print('='*50)\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”— Task 2: Link Prediction (Author Collaboration Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge splits created:\n",
      "Train: 53,616 | Val: 6,701 | Test: 6,701\n"
     ]
    }
   ],
   "source": [
    "# Create edge splits\n",
    "num_edges = data.edge_index.shape[1]\n",
    "perm = torch.randperm(num_edges)\n",
    "\n",
    "num_val = int(0.1 * num_edges)\n",
    "num_test = int(0.1 * num_edges)\n",
    "\n",
    "train_edges = data.edge_index[:, perm[num_val + num_test:]]\n",
    "val_pos = data.edge_index[:, perm[:num_val]]\n",
    "test_pos = data.edge_index[:, perm[num_val:num_val + num_test]]\n",
    "\n",
    "def sample_negative_edges(num_samples, existing_edges):\n",
    "    edge_set = set(map(tuple, existing_edges.t().tolist()))\n",
    "    neg_edges = []\n",
    "    while len(neg_edges) < num_samples:\n",
    "        src = torch.randint(0, data.num_nodes, (1,)).item()\n",
    "        dst = torch.randint(0, data.num_nodes, (1,)).item()\n",
    "        if src != dst and (src, dst) not in edge_set:\n",
    "            neg_edges.append([src, dst])\n",
    "    return torch.tensor(neg_edges, dtype=torch.long).t()\n",
    "\n",
    "val_neg = sample_negative_edges(num_val, data.edge_index)\n",
    "test_neg = sample_negative_edges(num_test, data.edge_index)\n",
    "\n",
    "print(f'Edge splits created:')\n",
    "print(f'Train: {train_edges.shape[1]:,} | Val: {val_pos.shape[1]:,} | Test: {test_pos.shape[1]:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_link_prediction(model, data, train_edges, val_pos, val_neg, test_pos, test_neg, epochs=50, lr=0.01):\n",
    "    model = model.to(device)\n",
    "    data_train = Data(x=data.x.to(device), edge_index=train_edges.to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_auc': []}\n",
    "    best_val_auc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        z = model(data_train.x, data_train.edge_index)\n",
    "        \n",
    "        num_samples = min(5000, train_edges.shape[1])\n",
    "        pos_idx = torch.randperm(train_edges.shape[1])[:num_samples]\n",
    "        pos_edges = train_edges[:, pos_idx].to(device)\n",
    "        neg_edges = sample_negative_edges(num_samples, train_edges).to(device)\n",
    "        \n",
    "        pos_score = (z[pos_edges[0]] * z[pos_edges[1]]).sum(dim=1)\n",
    "        neg_score = (z[neg_edges[0]] * z[neg_edges[1]]).sum(dim=1)\n",
    "        \n",
    "        loss = -torch.log(torch.sigmoid(pos_score) + 1e-15).mean() - torch.log(1 - torch.sigmoid(neg_score) + 1e-15).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        history['train_loss'].append(loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                z = model(data_train.x, data_train.edge_index)\n",
    "                pos_score = (z[val_pos.to(device)[0]] * z[val_pos.to(device)[1]]).sum(dim=1)\n",
    "                neg_score = (z[val_neg.to(device)[0]] * z[val_neg.to(device)[1]]).sum(dim=1)\n",
    "                scores = torch.cat([pos_score, neg_score]).cpu().numpy()\n",
    "                labels = np.concatenate([np.ones(len(pos_score)), np.zeros(len(neg_score))])\n",
    "                val_auc = roc_auc_score(labels, scores)\n",
    "                history['val_auc'].append(val_auc)\n",
    "                if val_auc > best_val_auc:\n",
    "                    best_val_auc = val_auc\n",
    "                print(f'Epoch {epoch+1}: Loss={loss.item():.4f}, Val AUC={val_auc:.4f}')\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(data_train.x, data_train.edge_index)\n",
    "        pos_score = (z[test_pos.to(device)[0]] * z[test_pos.to(device)[1]]).sum(dim=1)\n",
    "        neg_score = (z[test_neg.to(device)[0]] * z[test_neg.to(device)[1]]).sum(dim=1)\n",
    "        scores = torch.cat([pos_score, neg_score]).cpu().numpy()\n",
    "        labels = np.concatenate([np.ones(len(pos_score)), np.zeros(len(neg_score))])\n",
    "        test_auc = roc_auc_score(labels, scores)\n",
    "    \n",
    "    print(f'Final: Val AUC={best_val_auc:.4f}, Test AUC={test_auc:.4f}\\n')\n",
    "    return history, test_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GAT for Link Prediction...\n",
      "Epoch 10: Loss=0.8345, Val AUC=0.9918\n",
      "Epoch 20: Loss=0.7917, Val AUC=0.9940\n",
      "Epoch 30: Loss=0.7505, Val AUC=0.9965\n",
      "Epoch 40: Loss=0.7484, Val AUC=0.9973\n",
      "Epoch 50: Loss=0.7433, Val AUC=0.9976\n",
      "Final: Val AUC=0.9976, Test AUC=0.9977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training GAT for Link Prediction...')\n",
    "gat_link = GAT(data.num_features, 128, 64)\n",
    "gat_link_hist, gat_test_auc = train_link_prediction(gat_link, data, train_edges, val_pos, val_neg, test_pos, test_neg, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GCN for Link Prediction...\n",
      "Epoch 10: Loss=0.9597, Val AUC=0.9551\n",
      "Epoch 20: Loss=0.8999, Val AUC=0.9686\n",
      "Epoch 30: Loss=0.8253, Val AUC=0.9885\n",
      "Epoch 40: Loss=0.7770, Val AUC=0.9927\n",
      "Epoch 50: Loss=0.7682, Val AUC=0.9958\n",
      "Final: Val AUC=0.9958, Test AUC=0.9966\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training GCN for Link Prediction...')\n",
    "gcn_link = GCN(data.num_features, 128, 64)\n",
    "gcn_link_hist, gcn_test_auc = train_link_prediction(gcn_link, data, train_edges, val_pos, val_neg, test_pos, test_neg, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GraphSAGE for Link Prediction...\n",
      "Epoch 10: Loss=0.9955, Val AUC=0.9440\n",
      "Epoch 20: Loss=0.9554, Val AUC=0.9527\n",
      "Epoch 30: Loss=0.8983, Val AUC=0.9779\n",
      "Epoch 40: Loss=0.8221, Val AUC=0.9872\n",
      "Epoch 50: Loss=0.8025, Val AUC=0.9939\n",
      "Final: Val AUC=0.9939, Test AUC=0.9942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Training GraphSAGE for Link Prediction...')\n",
    "sage_link = GraphSAGE(data.num_features, 128, 64)\n",
    "sage_link_hist, sage_test_auc = train_link_prediction(sage_link, data, train_edges, val_pos, val_neg, test_pos, test_neg, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "LINK PREDICTION RESULTS\n",
      "==================================================\n",
      "    Model  Test AUC\n",
      "      GAT  0.997734\n",
      "      GCN  0.996558\n",
      "GraphSAGE  0.994213\n"
     ]
    }
   ],
   "source": [
    "results_link = pd.DataFrame({'Model': ['GAT', 'GCN', 'GraphSAGE'], 'Test AUC': [gat_test_auc, gcn_test_auc, sage_test_auc]})\n",
    "print('='*50)\n",
    "print('LINK PREDICTION RESULTS')\n",
    "print('='*50)\n",
    "print(results_link.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All models saved!\n"
     ]
    }
   ],
   "source": [
    "Path('saved_models').mkdir(exist_ok=True)\n",
    "torch.save({\n",
    "    'gat_node': gat.state_dict(),\n",
    "    'gcn_node': gcn.state_dict(),\n",
    "    'sage_node': sage.state_dict(),\n",
    "    'gat_link': gat_link.state_dict(),\n",
    "    'gcn_link': gcn_link.state_dict(),\n",
    "    'sage_link': sage_link.state_dict(),\n",
    "    'results_node': results.to_dict(),\n",
    "    'results_link': results_link.to_dict()\n",
    "}, 'saved_models/aminer_models.pt')\n",
    "print('âœ… All models saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Complete!\n",
    "\n",
    "You've successfully trained GNN models on **real AMiner author network data**!\n",
    "\n",
    "**What the models learned:**\n",
    "- ðŸŽ¯ Predict research fields based on author topics and collaboration patterns\n",
    "- ðŸ”— Predict potential author collaborations based on shared research interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL SUMMARY - AMiner Author Network Analysis\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ RESEARCH FIELD CLASSIFICATION\n",
      "------------------------------------------------------------\n",
      "    Model  Test Accuracy\n",
      "      GAT         0.6935\n",
      "      GCN         0.6900\n",
      "GraphSAGE         0.6895\n",
      "\n",
      "ðŸ”— AUTHOR COLLABORATION PREDICTION\n",
      "------------------------------------------------------------\n",
      "    Model  Test AUC\n",
      "      GAT  0.997734\n",
      "      GCN  0.996558\n",
      "GraphSAGE  0.994213\n",
      "\n",
      "============================================================\n",
      "âœ… All tasks completed successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print('\\n' + '='*60)\n",
    "print('FINAL SUMMARY - AMiner Author Network Analysis')\n",
    "print('='*60)\n",
    "print('\\nðŸŽ¯ RESEARCH FIELD CLASSIFICATION')\n",
    "print('-' * 60)\n",
    "print(results.to_string(index=False))\n",
    "print('\\nðŸ”— AUTHOR COLLABORATION PREDICTION')\n",
    "print('-' * 60)\n",
    "print(results_link.to_string(index=False))\n",
    "print('\\n' + '='*60)\n",
    "print('âœ… All tasks completed successfully!')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
